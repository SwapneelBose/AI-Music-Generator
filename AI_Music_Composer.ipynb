{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Generates Beethoven-style piano music from classical MIDI dataset\n",
        "Developed a deep learning model that generates original piano music sequences by training an LSTM neural network on classical MIDI files. The model predicts the next note in a sequence and converts the predictions into playable MIDI and WAV files"
      ],
      "metadata": {
        "id": "j2VoKJ8HrAsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing required libraries\n",
        "PyTorch for modeling\n",
        "\n",
        "music21 for processing MIDI files\n",
        "\n",
        "midi2audio for converting MIDI â†’ WAV\n",
        "\n",
        "kaggle API to download the dataset"
      ],
      "metadata": {
        "id": "bhyM4VtKpSod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fluidsynth"
      ],
      "metadata": {
        "id": "fmuX3H59lddE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install music21\n",
        "!pip install midi2audio\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "T2AHlrkfheYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading Classical Music MIDI Dataset"
      ],
      "metadata": {
        "id": "cSrFP-QppkYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "YP821pPVht9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!ls -l /root/.kaggle/"
      ],
      "metadata": {
        "id": "3MaTqyfQivGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/root/.kaggle\"\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Wcg6fYuKij0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unzip the Classical Music MIDI dataset:"
      ],
      "metadata": {
        "id": "6OoA431fpuOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d soumikrakshit/classical-music-midi\n",
        "!unzip -q classical-music-midi.zip -d midi_files\n",
        "\n",
        "import os\n",
        "midi_files = [f for f in os.listdir('midi_files') if f.endswith('.mid')]\n",
        "print(f\"Found {len(midi_files)} MIDI files. Sample files: {midi_files[:5]}\")\n"
      ],
      "metadata": {
        "id": "rS2g_W_AjDHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Listing all folders/files inside midi_files\n",
        "for root, dirs, files in os.walk('midi_files'):\n",
        "    print(f\"Folder: {root}, Files: {len(files)}\")\n",
        "    if len(files) > 0:\n",
        "        print(\"Sample files:\", files[:5])"
      ],
      "metadata": {
        "id": "-AX64-hRjOUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing MIDI files into notes"
      ],
      "metadata": {
        "id": "zSLopDHPqT-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import converter, instrument, note, chord\n",
        "import glob\n",
        "\n",
        "notes = []\n",
        "\n",
        "# Adjust path to the folder containing MIDI files\n",
        "midi_files = glob.glob(\"midi_files/beeth/*.mid\")\n",
        "print(f\"Found {len(midi_files)} MIDI files.\")\n",
        "\n",
        "# Parse all MIDI files and extract notes/chords\n",
        "for file in midi_files:\n",
        "    midi = converter.parse(file)\n",
        "    parts = instrument.partitionByInstrument(midi)\n",
        "    if parts:  # If instruments are separated\n",
        "        notes_to_parse = parts.parts[0].recurse()\n",
        "    else:\n",
        "        notes_to_parse = midi.flat.notes\n",
        "\n",
        "    for element in notes_to_parse:\n",
        "        if isinstance(element, note.Note):\n",
        "            notes.append(str(element.pitch))\n",
        "        elif isinstance(element, chord.Chord):\n",
        "            notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "print(f\"Total notes extracted: {len(notes)}\")\n",
        "print(f\"Unique notes: {len(set(notes))}\")"
      ],
      "metadata": {
        "id": "enupBvz3jVe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create i/o sequences"
      ],
      "metadata": {
        "id": "jfcTX-Rxqbce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sequence_length = 50\n",
        "\n",
        "# Map notes to integers\n",
        "pitchnames = sorted(set(notes))\n",
        "n_vocab = len(pitchnames)\n",
        "note_to_int = {note: number for number, note in enumerate(pitchnames)}\n",
        "\n",
        "network_input = []\n",
        "network_output = []\n",
        "\n",
        "for i in range(len(notes) - sequence_length):\n",
        "    seq_in = notes[i:i + sequence_length]\n",
        "    seq_out = notes[i + sequence_length]\n",
        "    network_input.append([note_to_int[n] for n in seq_in])\n",
        "    network_output.append(note_to_int[seq_out])\n",
        "\n",
        "print(f\"Number of sequences: {len(network_input)}\")\n",
        "\n",
        "# Reshape for LSTM input\n",
        "network_input = np.reshape(network_input, (len(network_input), sequence_length, 1))\n",
        "network_input = network_input / float(n_vocab)\n",
        "network_output = np.array(network_output)\n",
        "\n",
        "print(\"network_input shape:\", network_input.shape)\n",
        "print(\"network_output shape:\", network_output.shape)"
      ],
      "metadata": {
        "id": "yB1jstN7kXi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LSTM model"
      ],
      "metadata": {
        "id": "fKr8OXVQqhFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Dataset class\n",
        "class MusicDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs):\n",
        "        self.X = torch.tensor(inputs, dtype=torch.float32)\n",
        "        self.y = torch.tensor(outputs, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = MusicDataset(network_input, network_output)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# LSTM model\n",
        "class MusicLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=2):\n",
        "        super(MusicLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = MusicLSTM(input_size=1, hidden_size=256, output_size=n_vocab).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "D0UNm22bkdmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "gb8-WyL8qliZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in dataloader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "8b262Yd8kpEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate music sequences"
      ],
      "metadata": {
        "id": "LeBkx6AjqqTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import stream, note, chord\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "model.eval()  # switch to evaluation mode\n",
        "\n",
        "# Pick a random seed sequence from the input\n",
        "start_idx = np.random.randint(0, len(network_input)-1)\n",
        "pattern = network_input[start_idx].tolist()\n",
        "\n",
        "output_notes = []\n",
        "\n",
        "# Generate 500 notes\n",
        "for note_index in range(1000):\n",
        "    prediction_input = torch.tensor([pattern], dtype=torch.float32).to(device)\n",
        "    prediction = model(prediction_input)\n",
        "    import torch.nn.functional as F\n",
        "\n",
        "    temperature = 1.0  # adjust creativity (0.7(repetitive) or 1.0 (safe) or 1.2(risky))\n",
        "    prediction_probs = F.softmax(prediction / temperature, dim=1)\n",
        "    index = torch.multinomial(prediction_probs, num_samples=1).item()\n",
        "    result = pitchnames[index]\n",
        "\n",
        "    output_notes.append(result)\n",
        "\n",
        "    # Append new note and remove first to maintain sequence length\n",
        "    pattern.append([index/float(n_vocab)])\n",
        "    pattern = pattern[1:]\n",
        "\n",
        "print(f\"Generated {len(output_notes)} notes.\")"
      ],
      "metadata": {
        "id": "XPKkebSUlI9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Converting generated notes to MIDI and play"
      ],
      "metadata": {
        "id": "bR6pKv25quXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from music21 import stream, note, chord\n",
        "from midi2audio import FluidSynth\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Convert notes/chords to music21 stream\n",
        "offset = 0\n",
        "output_stream = stream.Stream()\n",
        "\n",
        "for pattern_note in output_notes:\n",
        "    # Handle chords\n",
        "    if '.' in pattern_note or pattern_note.isdigit():\n",
        "        notes_in_chord = pattern_note.split('.')\n",
        "        chord_notes = []\n",
        "        for n in notes_in_chord:\n",
        "            new_note = note.Note(int(n))\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            chord_notes.append(new_note)\n",
        "        new_chord = chord.Chord(chord_notes)\n",
        "        new_chord.offset = offset\n",
        "        output_stream.append(new_chord)\n",
        "    else:\n",
        "        new_note = note.Note(pattern_note)\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = instrument.Piano()\n",
        "        output_stream.append(new_note)\n",
        "    offset += 0.5\n",
        "\n",
        "# Save as MIDI\n",
        "output_stream.write('midi', fp='generated_music.mid')\n",
        "\n",
        "# Convert to WAV and play\n",
        "fs = FluidSynth()\n",
        "fs.midi_to_audio('generated_music.mid', 'generated_music.wav')\n",
        "\n",
        "Audio('generated_music.wav')"
      ],
      "metadata": {
        "id": "Y8wHaGTmlRRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}